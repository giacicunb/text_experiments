{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Explained, a comparison and code tutorial\n",
    "Disponível em https://medium.com/@dcameronsteinke/tf-idf-vs-word-embedding-a-comparison-and-code-tutorial-5ba341379ab0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outras fontes interessantes:\n",
    "- Paper Mikolov: https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "- https://medium.com/explorations-in-language-and-learning/online-learning-of-word-embeddings-7c2889c99704\n",
    "- https://medium.com/explorations-in-language-and-learning/understanding-word-vectors-f5f9e9fdef98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicts de word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testes feitos com o arquivo compscience_papers.csv, usaremos aqui o dicionário de word embeddings em inglês da FastText, desenvolvido pelo centro de Pesquisa em AI do Facebook. Usaremos o wiki-news-300d-1M.vec lookup dictionary, o qual contém mapeamento 300-dimensional de 1 milhão de palavras únicas - para economizar memória, os testes foram feitos usando apenas as 100.000 palavras mais comuns, sendo as restantes ignoradas. O arquivo está disponível para download em https://fasttext.cc/docs/en/english-vectors.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.016, -0.0003, -0.1684, 0.0899, -0.02, -0.0093, 0.0482, -0.0308, -0.0451, 0.0006, 0.168, 0.0965, 0.3061, -0.0411, 0.0296, -0.0463, 0.0325, -0.0703, 0.0222, -0.1404, -0.2638, -0.0134, 0.1277, 0.1227, 0.1803, -0.0192, 0.0353, 0.1214, 0.1509, -0.0861, 0.0976, -0.0255, -0.0276, -0.1556, -0.0739, 0.0543, -0.067, -0.003, 0.1515, 0.0608, 0.033, 0.0747, 0.0009, 0.055, 0.0048, -0.0132, -0.0262, -0.1804, 0.0805, 0.0464, -0.0159, -0.0302, -0.6785, 0.1632, 0.0103, 0.0655, -0.0843, 0.0227, 0.0335, -0.0356, -0.0638, -0.1111, -0.0017, 0.0978, 0.0565, -0.0352, 0.0395, 0.1867, 0.079, -0.1234, 0.0186, 0.089, 0.1631, 0.0783, 0.0561, 0.1447, -0.0251, 0.1376, -0.0079, -0.0239, 0.0218, 0.1494, -0.0191, -0.2479, -0.0499, 0.0516, -0.1298, -0.0648, 0.2738, 0.0078, 0.0171, -0.0372, 0.077, -0.1167, -0.0377, -0.0432, 0.0186, 0.0209, -0.0167, 0.0345, -0.1472, 0.0122, -0.053, -0.0073, 0.1029, 0.0283, -0.1264, 0.0066, -0.0579, 0.1004, -0.1225, 0.0247, 0.0808, -0.0399, -0.0108, 0.0043, 0.0184, 0.0488, -0.174, -0.3181, -0.129, 0.0783, -0.1382, 0.0573, 0.0325, 0.1704, -0.1343, 0.0037, -0.0304, 0.0407, 0.2318, 0.0393, 0.1592, -0.0602, 0.0273, 0.1087, -0.0189, -0.103, -0.1526, -0.0783, -0.1257, 0.1261, -0.0832, 0.1561, -0.2254, -0.1236, -0.1028, 0.0583, -0.0299, 0.1361, -0.0436, -0.0158, -0.0121, 0.1076, -0.0177, -0.0889, -0.0053, -0.0457, -0.0317, -0.1454, -0.1237, -0.0886, -0.0162, -0.1603, 0.0505, 0.15, 0.0697, -0.0715, -0.0245, -0.0099, -0.1832, 0.0413, -0.0251, 0.0845, 0.0284, -0.1314, 0.3021, -0.1812, -0.0738, -0.0999, -0.0298, 0.1508, -0.0443, 0.1709, -0.0549, -0.1333, -0.0046, 0.0395, -0.254, -0.0696, 0.019, -0.0414, 0.0729, 0.0556, -0.0921, 0.0986, 0.0049, -0.1271, 0.0958, -0.114, -0.0224, 0.02, -0.0104, -0.0111, 0.0064, 0.0619, -0.1497, -0.1185, 0.0554, 0.0396, 0.0309, 0.0395, -0.0876, -0.0306, -0.1778, 0.1257, -0.157, 0.1452, -0.1522, 0.0098, 0.0993, -0.0046, 0.0523, -0.0985, 0.0832, -0.2352, 0.0205, 0.1426, -0.0085, -0.0316, -0.0255, 0.0685, 0.3141, -0.0637, 0.0705, -0.1557, -0.2177, 0.0138, -0.2602, 0.0435, -0.1156, -0.0142, -0.1423, -0.2142, -0.0231, -0.0729, 0.1277, -0.1052, -0.1444, 0.4128, 0.1017, -0.1077, 0.0722, -0.0629, -0.0949, -0.1079, -0.0631, -0.0389, -0.0351, 0.0992, 0.0205, 0.2151, 0.0977, -0.0359, -0.4316, 0.1129, -0.1438, 0.0053, -0.1333, -0.1541, 0.0691, 0.115, -0.0566, -0.005, 0.1207, -0.0611, -0.0474, -0.1151, -0.0287, 0.1378, -0.0729, -0.0217, 0.1108, 0.0277, -0.0201, -0.2236, -0.0125, -0.0693, 0.0234, -0.0214, 0.0694, -0.0507, -0.0549, -0.0927, 0.0702, 0.1719, -0.0137, 0.0068, 0.1336, 0.0286]\n"
     ]
    }
   ],
   "source": [
    "# Loading the data file from local download\n",
    "path_fastText = 'wiki-news-300d-1M.vec'\n",
    "dictionary = open(path_fastText, 'r', encoding='utf-8',\n",
    "                  newline='\\n', errors='ignore')\n",
    "embeds = {}\n",
    "for line in dictionary:\n",
    "    tokens = line.rstrip().split(' ')\n",
    "    embeds[tokens[0]] = [float(x) for x in tokens[1:]]\n",
    "    \n",
    "    if len(embeds) == 100000:\n",
    "        break\n",
    "print(embeds['car'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para os testes feitos com os dados fornecidos pelo paper Inferring the source of official texts: can SVM beat ULMFiT?, disponível em https://cic.unb.br/~teodecampos/KnEDLe/propor2020/, usaremos o dicionário de embeddings em língua portuguesa fornecido pelo NILC - http://www.nilc.icmc.usp.br/embeddings. Usaremos o FastText CBOW 300 dimensões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.1796, 0.19209, 0.22359, -2.6106, -0.12218, 0.032726, 2.3504, 0.22662, 1.4124, 1.0825, -0.85295, 0.29017, 1.9458, 1.0684, -0.35541, 2.2881, -0.69868, 0.087266, 0.83, 2.3799, -1.0629, -0.67116, 0.075786, -2.1696, 2.9464, 0.10431, 0.39483, 0.34337, 0.62104, -0.44857, -0.99443, -0.089758, -0.52677, -2.332, -1.5584, 0.31164, -1.3988, -3.7487, -1.0172, -0.37468, -1.3402, 1.6693, -1.4098, 1.4323, -0.87958, -1.0398, 2.4174, -0.2133, 2.1066, 1.0972, -0.2813, 0.57358, -1.2151, -1.3656, 1.809, 0.21582, 1.0618, -0.34813, -1.7238, 0.27825, 0.10802, 0.83614, -3.3179, 0.12296, 1.3057, -0.15216, 0.58171, -1.5202, 0.35706, -2.6664, -2.9915, 0.67336, -1.2807, -0.30292, -1.1132, 0.62384, -2.7854, -0.87867, -2.2291, 1.9846, -2.3804, 0.562, 2.6336, -0.56135, -3.8398, -2.2169, -2.1987, -0.23831, -1.0293, 0.54459, -0.081219, -1.1411, 1.8342, -1.2903, 0.065219, 1.5573, -1.4097, 5.0478, -0.86079, -0.29849, -0.80884, 0.58545, -0.58787, -1.7649, 1.0095, 0.54806, 3.687, 3.1301, 1.2827, -0.78299, -0.30358, 0.093236, -0.40512, -2.0181, 2.4951, -0.81353, -0.90628, 1.7713, -2.9294, 0.15402, -1.3697, 0.5756, -0.90683, 1.8319, 1.4335, -0.54358, -0.77404, -1.0415, 0.93459, -0.19306, 0.34646, -0.87089, 1.2964, 2.8633, -2.2209, 1.4565, -0.48241, 0.15615, -1.7325, -1.3903, 2.7735, 0.92647, -2.6751, -2.227, -3.5377, -1.3921, -2.0837, -2.3775, -0.058728, -0.77097, 1.0388, 1.2649, -1.2854, -0.25772, -0.57621, 0.30348, -0.14684, 1.2718, 1.3582, 1.4718, -1.0667, 2.2142, -0.3236, 1.0765, 1.549, -1.1085, -0.54298, -1.6197, -0.52051, -1.3354, -0.79908, 3.0767, 0.73561, -0.97507, -0.62256, -2.7077, -1.276, 1.4895, -1.4214, -1.2142, 2.9405, 0.059907, -0.5569, 0.99031, -0.584, -1.1033, 2.8029, -0.35555, -0.61177, 0.45168, 0.58676, 0.49072, -1.2189, 0.29576, -0.78148, -0.37354, 0.83801, -2.0058, -2.7276, 0.48452, -1.8534, -1.5133, 3.2552, -0.39481, -0.44087, -1.2029, -0.78091, -1.5259, -1.2169, -2.1231, -0.64151, -0.22709, 0.72013, -0.40038, -1.3359, -2.295, 0.18432, -0.93943, -2.0317, 0.4232, -0.79609, -2.9138, -1.0948, 0.84477, -0.29411, 1.0235, 1.3993, 2.2356, 0.10207, -2.218, -2.2072, 0.029636, 1.4536, 0.27108, 0.11432, -2.5009, -2.2515, 1.2968, 6.134, 1.6308, 0.74981, -1.7925, -0.23319, -1.3693, -0.80141, -4.0001, 2.1292, 0.94441, 1.3555, 1.7527, 2.5303, -1.0127, 2.0018, 1.1698, 2.4555, 1.6198, 0.099997, 2.495, 2.091, -1.8268, -1.5661, -1.9517, -0.097074, 1.0707, -0.86812, -1.4722, 1.2477, -2.9397, -1.4012, 0.98404, -0.049854, -0.93873, 1.3462, -2.0158, 0.86476, -0.56212, 0.35808, 0.77377, 1.0031, 0.029788, 1.8929, -0.21989, -0.51853, 0.90876, -0.41044, -0.93653, -0.17206, 1.2777, -2.1318, 0.97462, 3.4671, 2.6029, 3.3108, 3.8534, 1.0045, 1.2036, 0.78669, -0.79523, -1.1357, 0.61145]\n"
     ]
    }
   ],
   "source": [
    "# Loading the data file from local download\n",
    "path_fastTextPT = 'cbow_s300.txt'\n",
    "dictionary = open(path_fastTextPT, 'r', encoding='utf-8',\n",
    "                  newline='\\n', errors='ignore')\n",
    "embedsPT = {}\n",
    "for line in dictionary:\n",
    "    tokens = line.rstrip().split(' ')\n",
    "    embedsPT[tokens[0]] = [float(x) for x in tokens[1:]]\n",
    "    \n",
    "    if len(embedsPT) == 100000:\n",
    "        break\n",
    "print(embedsPT['carro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import de ambos os arquivos csv:\n",
    " - train.csv, do trabalho de Pedro e\n",
    " - compscience_papers.csv, do repositório de testes da equipe DataAnnotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                label  \\\n",
      "10  SECRETARIA DE ESTADO DE DESENVOLVIMENTO URBANO...   \n",
      "11  SECRETARIA DE ESTADO DE FAZENDA, PLANEJAMENTO,...   \n",
      "12                      SECRETARIA DE ESTADO DE SAÚDE   \n",
      "13          SECRETARIA DE ESTADO DE SEGURANÇA PÚBLICA   \n",
      "\n",
      "                                                 text  is_valid  \n",
      "10  O Termo de Recebimento Definitivo declarará fo...     False  \n",
      "11  O DISTRITO FEDERAL, por intermédio da Diretori...     False  \n",
      "12  O SECRETÁRIO DE ESTADO DE SAÚDE DO DISTRITO FE...     False  \n",
      "13  O DIRETOR-GERAL DO DEPARTAMENTO DE TRÂNSITO DO...     False  \n"
     ]
    }
   ],
   "source": [
    "path_to_text = '/home/mstauffer/Documentos/UnB/9º Semestre/KnEDle/sprints/5_27_maio-03_junho/luz_de_araujo_etal_propor2020/data/clean/train.csv'\n",
    "data_peter = pd.read_csv(path_to_text, encoding='utf8')#[['v1', 'v2']]\n",
    "# Creating the feature set and label set\n",
    "textPT = data_peter['text']\n",
    "labelPT = data_peter['label']\n",
    "print(data_peter[10:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  label\n",
      "10  INFORMATION EXTRACTION AS A BASIS FOR PORTABLE...      3\n",
      "11  Footprint-Based Retrieval\\n\\nBarry Smyth & Eli...      1\n",
      "12  CATEGORY: Artificial Intelligence Learning Hum...      3\n",
      "13  Distributed Energy­conserving Routing Protocol...      3\n"
     ]
    }
   ],
   "source": [
    "path_to_text = '/home/mstauffer/Documentos/UnB/9º Semestre/KnEDle/sprints/compscience_papers/compscience_papers.csv'\n",
    "data = pd.read_csv(path_to_text, encoding='utf8')#[['v1', 'v2']]\n",
    "# Creating the feature set and label set\n",
    "text = data['text']\n",
    "label = data['label']\n",
    "print(data[10:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento embeddings\n",
    "Para tratar as word embeddings, usaremos um método de pré-processamento da lib Keras que converte um texto em uma sequência de palavras (os famosos tokens). Pontuação e capitalização também serão removidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tratamento dados Pedro (Em português)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(717, 6000)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "array_length = 20 * 300\n",
    "embedding_featuresPT = pd.DataFrame()\n",
    "for document in textPT:\n",
    "    # Saving the first 20 words of the document as a sequence\n",
    "    words = text_to_word_sequence(document)[0:20] \n",
    "    \n",
    "    # Retrieving the vector representation of each word and \n",
    "    # appending it to the feature vector \n",
    "    feature_vector = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            feature_vector = np.append(feature_vector, \n",
    "                                       np.array(embedsPT[word]))\n",
    "        except KeyError:\n",
    "            # In the event that a word is not included in our \n",
    "            # dictionary skip that word\n",
    "            pass\n",
    "    # If the text has less then 20 words, fill remaining vector with\n",
    "    # zeros\n",
    "    zeroes_to_add = array_length - len(feature_vector)\n",
    "    feature_vector = np.append(feature_vector, \n",
    "                               np.zeros(zeroes_to_add)\n",
    "                               ).reshape((1,-1))\n",
    "    \n",
    "    # Append the document feature vector to the feature table\n",
    "    embedding_featuresPT = embedding_featuresPT.append( \n",
    "                                     pd.DataFrame(feature_vector))\n",
    "print(embedding_featuresPT.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tratamento dados Data Annotation (Em inglês)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(682, 6000)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "array_length = 20 * 300\n",
    "embedding_features = pd.DataFrame()\n",
    "for document in text:\n",
    "    # Saving the first 20 words of the document as a sequence\n",
    "    words = text_to_word_sequence(document)[0:20] \n",
    "    \n",
    "    # Retrieving the vector representation of each word and \n",
    "    # appending it to the feature vector \n",
    "    feature_vector = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            feature_vector = np.append(feature_vector, \n",
    "                                       np.array(embeds[word]))\n",
    "        except KeyError:\n",
    "            # In the event that a word is not included in our \n",
    "            # dictionary skip that word\n",
    "            pass\n",
    "    # If the text has less then 20 words, fill remaining vector with\n",
    "    # zeros\n",
    "    zeroes_to_add = array_length - len(feature_vector)\n",
    "    feature_vector = np.append(feature_vector, \n",
    "                               np.zeros(zeroes_to_add)\n",
    "                               ).reshape((1,-1))\n",
    "    \n",
    "    # Append the document feature vector to the feature table\n",
    "    embedding_features = embedding_features.append( \n",
    "                                     pd.DataFrame(feature_vector))\n",
    "print(embedding_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criamos tabelas TFIDF para baseline de comparação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TFIDF dados Pedro (Em português)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(717, 6000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = list(textPT)\n",
    "tfidfPT = TfidfVectorizer(max_features = 6000) \n",
    "tfidfPT.fit(corpus)\n",
    "tfidf_featuresPT = tfidfPT.transform(corpus)\n",
    "print(tfidf_featuresPT.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TFIDF dados Data Annotation (Em inglês)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(682, 6000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = list(text)\n",
    "tfidf = TfidfVectorizer(max_features = 6000) \n",
    "tfidf.fit(corpus)\n",
    "tfidf_features = tfidf.transform(corpus)\n",
    "print(tfidf_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento dos labels via sklearn label encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Labels dados Pedro (Em português)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# Converting the labels from strings to binary\n",
    "le = LabelEncoder()\n",
    "le.fit(labelPT)\n",
    "labelPT = le.transform(labelPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Labels dados Data Annotation (Em inglês)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# Converting the labels from strings to binary\n",
    "le = LabelEncoder()\n",
    "le.fit(label)\n",
    "label = le.transform(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dos dados em treino e teste e instanciação dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 70/30 train test split\n",
    "train_percent = 0.7\n",
    "train_cutoff = int(np.floor(train_percent*len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split e instanciação dos modelos - dados Pedro (Em português)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mstauffer/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Word Embedding\n",
    "embeded_modelPT = LinearSVC(max_iter=20000)\n",
    "embeded_modelPT.fit(embedding_featuresPT[0 : train_cutoff], \n",
    "                  labelPT[0 : train_cutoff])\n",
    "embeded_predictionPT = embeded_modelPT.predict(\n",
    "                   embedding_featuresPT[train_cutoff + 1 : len(text)])\n",
    "# TF-IDF table\n",
    "tfidf_modelPT = LinearSVC(max_iter=20000)\n",
    "tfidf_modelPT.fit(tfidf_featuresPT[0 : train_cutoff], \n",
    "                  labelPT[0 : train_cutoff])\n",
    "tfidf_predictionPT = tfidf_modelPT.predict(\n",
    "                  tfidf_featuresPT[train_cutoff + 1 : len(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split e instaciação do modelo - dados Data Annotation (Em inglês)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 70/30 train test split\n",
    "train_percent = 0.7\n",
    "train_cutoff = int(np.floor(train_percent*len(text) ) )\n",
    "# Word Embedding\n",
    "embeded_model = LinearSVC(max_iter=20000)\n",
    "embeded_model.fit(embedding_features[0 : train_cutoff], \n",
    "                  label[0 : train_cutoff])\n",
    "embeded_prediction = embeded_model.predict(\n",
    "                   embedding_features[train_cutoff + 1 : len(text)])\n",
    "# TF-IDF table\n",
    "tfidf_model = LinearSVC(max_iter=20000)\n",
    "tfidf_model.fit(tfidf_features[0 : train_cutoff], \n",
    "                  label[0 : train_cutoff])\n",
    "tfidf_prediction = tfidf_model.predict(\n",
    "                  tfidf_features[train_cutoff + 1 : len(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represantação dos resultados em tabelas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tabela dados Pedro (Em português)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mstauffer/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "resultsPT = pd.DataFrame(index = ['Word Embedding', 'TF-IDF'], \n",
    "          columns = ['Precision', 'Recall', 'F1 score', 'support']\n",
    "          )\n",
    "resultsPT.loc['Word Embedding'] = precision_recall_fscore_support(\n",
    "          labelPT[train_cutoff + 1 : len(text)], \n",
    "          embeded_predictionPT, \n",
    "          average = 'weighted'\n",
    "          )\n",
    "resultsPT.loc['TF-IDF'] = precision_recall_fscore_support(\n",
    "          labelPT[train_cutoff + 1 : len(text)], \n",
    "          tfidf_predictionPT, \n",
    "          average = 'weighted'\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tabela dados Data Annotation (Em inglês)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(index = ['Word Embedding', 'TF-IDF'], \n",
    "          columns = ['Precision', 'Recall', 'F1 score', 'support']\n",
    "          )\n",
    "results.loc['Word Embedding'] = precision_recall_fscore_support(\n",
    "          label[train_cutoff + 1 : len(text)], \n",
    "          embeded_prediction, \n",
    "          average = 'weighted'\n",
    "          )\n",
    "results.loc['TF-IDF'] = precision_recall_fscore_support(\n",
    "          label[train_cutoff + 1 : len(text)], \n",
    "          tfidf_prediction, \n",
    "          average = 'weighted'\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabelas de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Resultados dados Pedro (Em português)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Word Embedding</th>\n",
       "      <td>0.835766</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.828399</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.904991</td>\n",
       "      <td>0.897059</td>\n",
       "      <td>0.894488</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Precision    Recall  F1 score support\n",
       "Word Embedding  0.835766  0.833333  0.828399    None\n",
       "TF-IDF          0.904991  0.897059  0.894488    None"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsPT.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Resultados dados Data Annotation (Em inglês)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Word Embedding</th>\n",
       "      <td>0.673325</td>\n",
       "      <td>0.681373</td>\n",
       "      <td>0.675881</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.966429</td>\n",
       "      <td>0.965686</td>\n",
       "      <td>0.965066</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Precision    Recall  F1 score support\n",
       "Word Embedding  0.673325  0.681373  0.675881    None\n",
       "TF-IDF          0.966429  0.965686  0.965066    None"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
